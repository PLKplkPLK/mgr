{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43633b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torchvision import transforms, datasets\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from utils.dataloaders import IMG_SIZE, CroppedImageDataset, is_valid_image\n",
    "from utils.class_names import class_names, class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dc96a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837a22f7",
   "metadata": {},
   "source": [
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b7f6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter: accept only files inside pictures_cropped\n",
    "training_data_transforms = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(\n",
    "            brightness=0.2,\n",
    "            contrast=0.2,\n",
    "            saturation=0.2,\n",
    "            hue=0.05,\n",
    "        ),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "dataset = datasets.ImageFolder(\n",
    "    root=\"../../data\",\n",
    "    transform=training_data_transforms,\n",
    "    is_valid_file=is_valid_image,\n",
    ")\n",
    "print('Number of classes: ', len(dataset.classes))\n",
    "print('Number of images: ', len(dataset))\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "if dataset.class_to_idx != class_to_idx:\n",
    "    raise ValueError('Mapping doesn\\'t match')\n",
    "\n",
    "df = pd.read_csv('../../y_clean_thin.csv', index_col=0).sample(frac=0.2).reset_index(drop=True)\n",
    "df_mega = pd.read_csv('../../megadetector_results.csv', index_col=0)\n",
    "df = df.merge(df_mega, on='image_path')\n",
    "df.image_path = '../../' + df.image_path\n",
    "df = df.dropna()\n",
    "\n",
    "val_ds = CroppedImageDataset(df)\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15eb450",
   "metadata": {},
   "source": [
    "### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ba9a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = [label for _, label in dataset.samples]\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "weights = torch.tensor(class_weights, dtype=torch.float32).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b8de26",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_epochs = 20\n",
    "polish_model = model(feat_extractor, len(class_names)).to(\"cuda\")\n",
    "\n",
    "optimizer = torch.optim.Adam(polish_model.classifier.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "scaler = GradScaler()\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(1, number_of_epochs+1):\n",
    "    # TRAIN\n",
    "    train_loss = 0\n",
    "    polish_model.train()\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "        \n",
    "        with autocast('cuda'):\n",
    "            logits = polish_model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # TEST\n",
    "    polish_model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            with autocast('cuda'):\n",
    "                logits = polish_model(images)\n",
    "                loss = criterion(logits, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print(f\"epoch {epoch}: train_loss={train_loss:.2f}  val_loss={val_loss:.2f}\")\n",
    "\n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "        best_state = polish_model.state_dict()\n",
    "\n",
    "checkpoint = {\n",
    "    'state_dict': best_state,\n",
    "    'class_names': class_names,\n",
    "    'feature_node': FEATURE_NODE,\n",
    "    'num_classes': len(class_names)\n",
    "}\n",
    "torch.save(checkpoint, \"speciesnet_polish_2_checkpoint.pt\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
