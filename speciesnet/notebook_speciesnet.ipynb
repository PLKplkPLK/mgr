{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "547a3f9e",
   "metadata": {},
   "source": [
    "Classify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a455f5a",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    ".\\venv\\Scripts\\activate\n",
    "python -m speciesnet.scripts.run_model --folders \".\\test_images\\\" --predictions_json \"results/output_speciesnet_pl.json\" --country POL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4771a3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "from torch import tensor\n",
    "from torchvision.transforms import transforms, InterpolationMode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d37524",
   "metadata": {},
   "source": [
    "#### One image checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e312c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "DFYOLO_NAME = 'deepfaune-yolov8s_960'\n",
    "DFYOLO_PATH = '../deepfaune/models/'\n",
    "DFYOLO_WEIGHTS = DFYOLO_PATH + 'deepfaune-yolov8s_960.pt'\n",
    "SPECIESNET_PATH = 'models/speciesnet-pytorch-v4.0.1a-v1/'\n",
    "SPECIESNET_MODEL = SPECIESNET_PATH + 'always_crop_99710272_22x8_v12_epoch_00148.pt'\n",
    "BACKBONE = 'efficientnetv2_m'  # not neccessary\n",
    "\n",
    "LABELS_FILE = 'models/speciesnet-pytorch-v4.0.1a-v1/always_crop_99710272_22x8_v12_epoch_00148.labels.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b27a747",
   "metadata": {},
   "outputs": [],
   "source": [
    "CROP_SIZE = 480 # default 480??\n",
    "DFYOLO_WIDTH = 960 # image width\n",
    "DFYOLO_THRES = 0.6\n",
    "DFYOLOHUMAN_THRES = 0.4 # boxes with human above this threshold are saved\n",
    "DFYOLOCOUNT_THRES = 0.6\n",
    "\n",
    "\n",
    "with open(LABELS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    txt_animalclasses = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "\n",
    "class Detector:\n",
    "    def __init__(self, name=DFYOLO_NAME, threshold=None, countthreshold=None, humanthreshold=None):\n",
    "        print(\"Using \"+DFYOLO_NAME+\" with weights at \"+DFYOLO_WEIGHTS+\", in resolution 960x960\")\n",
    "        self.yolo = YOLO(DFYOLO_WEIGHTS)\n",
    "        self.imgsz = DFYOLO_WIDTH\n",
    "        self.threshold = DFYOLO_THRES if threshold is None else threshold\n",
    "        self.countthreshold = DFYOLOCOUNT_THRES if countthreshold is None else countthreshold\n",
    "        self.humanthreshold = DFYOLOHUMAN_THRES if humanthreshold is None else humanthreshold\n",
    "\n",
    "    def bestBoxDetection(self, filename_or_imagecv):\n",
    "        try:\n",
    "            results = self.yolo(filename_or_imagecv, verbose=False, imgsz=self.imgsz)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File '{filename_or_imagecv}' not found\")\n",
    "            return None, 0, np.zeros(4), 0, []\n",
    "        except Exception as err:\n",
    "            print(err)\n",
    "            return None, 0, np.zeros(4), 0, []\n",
    "\n",
    "        # orig_img a numpy array (cv2) in BGR\n",
    "        imagecv = results[0].cpu().orig_img\n",
    "        detection = results[0].cpu().numpy().boxes\n",
    "\n",
    "        # Are there any relevant boxes?\n",
    "        if not len(detection.cls) or detection.conf[0] < self.threshold:\n",
    "            # No. Image considered as empty\n",
    "            return None, 0, np.zeros(4), 0, []\n",
    "        else:\n",
    "            # Yes. Non empty image\n",
    "            pass\n",
    "        # Is there a relevant animal box? \n",
    "        try:\n",
    "            # Yes. Selecting the best animal box\n",
    "            kbox = np.where((detection.cls==0) & (detection.conf>self.threshold))[0][0]\n",
    "        except IndexError:\n",
    "            # No: Selecting the best box for another category (human, vehicle)\n",
    "            kbox = 0\n",
    "\n",
    "        # categories are 1=animal, 2=person, 3=vehicle and the empty category 0=empty\n",
    "        category = int(detection.cls[kbox]) + 1\n",
    "        box = detection.xyxy[kbox] # xmin, ymin, xmax, ymax\n",
    "\n",
    "        # Is this an animal box ?\n",
    "        if category == 1:\n",
    "            # Yes: cropped image is required for classification\n",
    "            croppedimage = cropSquareCVtoPIL(imagecv, box.copy())\n",
    "        else: \n",
    "            # No: return none\n",
    "            return None, 0, np.zeros(4), 0, []\n",
    "        \n",
    "        ## animal count\n",
    "        if category == 1:\n",
    "            count = sum((detection.conf>self.countthreshold) & (detection.cls==0)) # only above a threshold\n",
    "        else:\n",
    "            count = 0\n",
    "        ## human boxes\n",
    "        ishuman = (detection.cls==1) & (detection.conf>=self.humanthreshold)\n",
    "        if any(ishuman==True):\n",
    "            humanboxes = detection.xyxy[ishuman,]\n",
    "        else:\n",
    "            humanboxes = []\n",
    "\n",
    "        return croppedimage, category, box, count, humanboxes\n",
    "\n",
    "\n",
    "def cropSquareCVtoPIL(imagecv, box):\n",
    "    x1, y1, x2, y2 = box\n",
    "    xsize = (x2-x1)\n",
    "    ysize = (y2-y1)\n",
    "    if xsize>ysize:\n",
    "        y1 = y1-int((xsize-ysize)/2)\n",
    "        y2 = y2+int((xsize-ysize)/2)\n",
    "    if ysize>xsize:\n",
    "        x1 = x1-int((ysize-xsize)/2)\n",
    "        x2 = x2+int((ysize-xsize)/2)\n",
    "    height, width, _ = imagecv.shape\n",
    "    croppedimagecv = imagecv[max(0,int(y1)):min(int(y2),height),max(0,int(x1)):min(int(x2),width)]\n",
    "    croppedimage = Image.fromarray(croppedimagecv[:,:,(2,1,0)]) # converted to PIL BGR image\n",
    "    return croppedimage\n",
    "\n",
    "\n",
    "class Classifier:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = torch.load(SPECIESNET_MODEL, map_location=self.device, weights_only=False)\n",
    "        self.model.eval()\n",
    "        print(f'Speciesnet loaded onto {self.device}')\n",
    "\n",
    "        # transform image to form usable by network\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.Resize(size=(CROP_SIZE, CROP_SIZE), interpolation=InterpolationMode.BICUBIC),\n",
    "            transforms.ToTensor()\n",
    "            # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def predictOnBatch(self, batchtensor, withsoftmax=True):\n",
    "        batchtensor = batchtensor.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(batchtensor)\n",
    "            preds = logits.softmax(dim=1) if withsoftmax else logits\n",
    "        return preds.cpu().numpy()\n",
    "\n",
    "    def preprocessImage(self, croppedimage):\n",
    "        return self.transforms(croppedimage).unsqueeze(dim=0)  # batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d787524",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = pd.read_csv('../y_clean.csv', index_col=0)\n",
    "images.RelativePath = images.RelativePath.str.replace('\\\\', '/')\n",
    "image = images.iloc[1000,:]\n",
    "image_path = '../../pictures/' + image.RelativePath + '/' + image.File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4aa7df7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "File            2023-07-22 19-22-33.JPG\n",
       "RelativePath     02_WYSZOWATKA/B/Lato/2\n",
       "species                         roedeer\n",
       "Name: 1000, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6557627e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using deepfaune-yolov8s_960 with weights at ../deepfaune/models/deepfaune-yolov8s_960.pt, in resolution 960x960\n",
      "Predicted class: 317171d7-d306-4e71-9a4a-33e62012076b;mammalia;cetartiodactyla;cervidae;capreolus;capreolus;european roe deer (confidence 0.910)\n"
     ]
    }
   ],
   "source": [
    "detector = Detector()\n",
    "classifier = Classifier()\n",
    "\n",
    "cropped_img, category, box, count, humanboxes = detector.bestBoxDetection(image_path)\n",
    "batch = classifier.preprocessImage(cropped_img)\n",
    "batch = batch.permute(0, 2, 3, 1).contiguous()\n",
    "preds = classifier.predictOnBatch(batch)\n",
    "top_idx = np.argmax(preds)\n",
    "print(f\"Predicted class: {txt_animalclasses[top_idx]} (confidence {preds[0][top_idx]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e548c4c5",
   "metadata": {},
   "source": [
    "#### PÄ™tla klasyfikacji"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4c0d8e",
   "metadata": {},
   "source": [
    "##### Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d4ab8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = torch.load(SPECIESNET_MODEL, map_location=self.device, weights_only=False)\n",
    "        self.model.eval()\n",
    "        self.model.half()\n",
    "        print(f'Speciesnet loaded onto {self.device}')\n",
    "\n",
    "        # transform image to form usable by network\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.Resize(size=(CROP_SIZE, CROP_SIZE), interpolation=InterpolationMode.BICUBIC),\n",
    "            transforms.ToTensor()\n",
    "            # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def predictOnBatch(self, batch_tensor, withsoftmax=True):\n",
    "        batch_tensor = batch_tensor.to(self.device).half()\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(batch_tensor)\n",
    "            preds = logits.softmax(dim=1) if withsoftmax else logits\n",
    "        return preds.cpu().numpy()\n",
    "\n",
    "    def preprocessImage(self, croppedimage):\n",
    "        return self.transforms(croppedimage).unsqueeze(dim=0)  # batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8671230a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using deepfaune-yolov8s_960 with weights at ../deepfaune/models/deepfaune-yolov8s_960.pt, in resolution 960x960\n",
      "Speciesnet loaded onto cuda\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "detector = Detector()\n",
    "classifier = Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45ba92eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "def collate_non_null(batch):\n",
    "    \"\"\"Collate function that filters out None entries.\"\"\"\n",
    "    return [item for item in batch if item[0] is not None]\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform):\n",
    "        self.df = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_path = '../../pictures/' + row['RelativePath'] + '/' + row['File']\n",
    "        croppedimage, _, _, _, _ = detector.bestBoxDetection(image_path)\n",
    "        if croppedimage is None:\n",
    "            return None, image_path\n",
    "        return self.transform(croppedimage), image_path\n",
    "\n",
    "images = pd.read_csv('../y_clean.csv', index_col=0)\n",
    "images.RelativePath = images.RelativePath.str.replace('\\\\', '/')\n",
    "\n",
    "# build dataset & dataloader\n",
    "dataset = ImageDataset(images, classifier.transforms)\n",
    "loader = DataLoader(dataset, batch_size=200, num_workers=0, pin_memory=True, collate_fn=collate_non_null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff162724",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({'image': [], 'detected_animal': [], 'confidence': []})\n",
    "for batch in loader:\n",
    "    if len(batch) == 0:\n",
    "        continue\n",
    "    tensors, paths = zip(*batch)\n",
    "    batch_tensor = torch.stack(tensors).to(classifier.device)\n",
    "    batch_tensor = batch_tensor.permute(0, 2, 3, 1).contiguous()\n",
    "    preds = classifier.predictOnBatch(batch_tensor)\n",
    "    top_idx = np.argmax(preds, axis=1)\n",
    "    for path, idx, conf in zip(paths, top_idx, preds[range(len(preds)), top_idx]):\n",
    "        output = txt_animalclasses[idx].rsplit(';')[-1]\n",
    "        results.loc[len(results)] = [path, output, conf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf5d1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5f9c83",
   "metadata": {},
   "source": [
    "In 5:30: 1250 images xD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56c8abf",
   "metadata": {},
   "source": [
    "##### Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b46c1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using deepfaune-yolov8s_960 with weights at ../deepfaune/models/deepfaune-yolov8s_960.pt, in resolution 960x960\n",
      "Speciesnet loaded onto cuda\n"
     ]
    }
   ],
   "source": [
    "CROP_SIZE = 480 # default 480??\n",
    "DFYOLO_WIDTH = 960 # image width\n",
    "DFYOLO_THRES = 0.6\n",
    "DFYOLOHUMAN_THRES = 0.4 # boxes with human above this threshold are saved\n",
    "DFYOLOCOUNT_THRES = 0.6\n",
    "\n",
    "\n",
    "with open(LABELS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    txt_animalclasses = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "\n",
    "class Detector:\n",
    "    def __init__(self, name=DFYOLO_NAME, threshold=None, countthreshold=None, humanthreshold=None):\n",
    "        print(\"Using \"+DFYOLO_NAME+\" with weights at \"+DFYOLO_WEIGHTS+\", in resolution 960x960\")\n",
    "        self.yolo = YOLO(DFYOLO_WEIGHTS)\n",
    "        self.imgsz = DFYOLO_WIDTH\n",
    "        self.threshold = DFYOLO_THRES if threshold is None else threshold\n",
    "        self.countthreshold = DFYOLOCOUNT_THRES if countthreshold is None else countthreshold\n",
    "        self.humanthreshold = DFYOLOHUMAN_THRES if humanthreshold is None else humanthreshold\n",
    "\n",
    "    def bestBoxDetection(self, filename_or_imagecv):\n",
    "        try:\n",
    "            results = self.yolo(filename_or_imagecv, verbose=False, imgsz=self.imgsz)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File '{filename_or_imagecv}' not found\")\n",
    "            return None, 0, np.zeros(4), 0, []\n",
    "        except Exception as err:\n",
    "            print(err)\n",
    "            return None, 0, np.zeros(4), 0, []\n",
    "\n",
    "        # orig_img a numpy array (cv2) in BGR\n",
    "        imagecv = results[0].cpu().orig_img\n",
    "        detection = results[0].cpu().numpy().boxes\n",
    "\n",
    "        # Are there any relevant boxes?\n",
    "        if not len(detection.cls) or detection.conf[0] < self.threshold:\n",
    "            # No. Image considered as empty\n",
    "            return None, 0, np.zeros(4), 0, []\n",
    "        else:\n",
    "            # Yes. Non empty image\n",
    "            pass\n",
    "        # Is there a relevant animal box? \n",
    "        try:\n",
    "            # Yes. Selecting the best animal box\n",
    "            kbox = np.where((detection.cls==0) & (detection.conf>self.threshold))[0][0]\n",
    "        except IndexError:\n",
    "            # No: Selecting the best box for another category (human, vehicle)\n",
    "            kbox = 0\n",
    "\n",
    "        # categories are 1=animal, 2=person, 3=vehicle and the empty category 0=empty\n",
    "        category = int(detection.cls[kbox]) + 1\n",
    "        box = detection.xyxy[kbox] # xmin, ymin, xmax, ymax\n",
    "\n",
    "        # Is this an animal box ?\n",
    "        if category == 1:\n",
    "            # Yes: cropped image is required for classification\n",
    "            croppedimage = cropSquareCVtoPIL(imagecv, box.copy())\n",
    "        else: \n",
    "            # No: return none\n",
    "            return None, 0, np.zeros(4), 0, []\n",
    "        \n",
    "        ## animal count\n",
    "        if category == 1:\n",
    "            count = sum((detection.conf>self.countthreshold) & (detection.cls==0)) # only above a threshold\n",
    "        else:\n",
    "            count = 0\n",
    "        ## human boxes\n",
    "        ishuman = (detection.cls==1) & (detection.conf>=self.humanthreshold)\n",
    "        if any(ishuman==True):\n",
    "            humanboxes = detection.xyxy[ishuman,]\n",
    "        else:\n",
    "            humanboxes = []\n",
    "\n",
    "        return croppedimage, category, box, count, humanboxes\n",
    "\n",
    "\n",
    "def cropSquareCVtoPIL(imagecv, box):\n",
    "    x1, y1, x2, y2 = box\n",
    "    xsize = (x2-x1)\n",
    "    ysize = (y2-y1)\n",
    "    if xsize>ysize:\n",
    "        y1 = y1-int((xsize-ysize)/2)\n",
    "        y2 = y2+int((xsize-ysize)/2)\n",
    "    if ysize>xsize:\n",
    "        x1 = x1-int((ysize-xsize)/2)\n",
    "        x2 = x2+int((ysize-xsize)/2)\n",
    "    height, width, _ = imagecv.shape\n",
    "    croppedimagecv = imagecv[max(0,int(y1)):min(int(y2),height),max(0,int(x1)):min(int(x2),width)]\n",
    "    croppedimage = Image.fromarray(croppedimagecv[:,:,(2,1,0)]) # converted to PIL BGR image\n",
    "    return croppedimage\n",
    "\n",
    "\n",
    "class Classifier:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = torch.load(SPECIESNET_MODEL, map_location=self.device, weights_only=False)\n",
    "        self.model.eval()\n",
    "        print(f'Speciesnet loaded onto {self.device}')\n",
    "\n",
    "        # transform image to form usable by network\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.Resize(size=(CROP_SIZE, CROP_SIZE), interpolation=InterpolationMode.BICUBIC),\n",
    "            transforms.ToTensor()\n",
    "            # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def predictOnBatch(self, batchtensor, withsoftmax=True):\n",
    "        batchtensor = batchtensor.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(batchtensor)\n",
    "            preds = logits.softmax(dim=1) if withsoftmax else logits\n",
    "        return preds.cpu().numpy()\n",
    "\n",
    "    def preprocessImage(self, croppedimage):\n",
    "        return self.transforms(croppedimage).unsqueeze(dim=0)  # batch dimension\n",
    "\n",
    "\n",
    "detector = Detector()\n",
    "classifier = Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc29732a",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = pd.read_csv('../y_clean.csv', index_col=0)\n",
    "images.RelativePath = images.RelativePath.str.replace('\\\\', '/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dd397c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({'image': [], 'detected_animal': [], 'confidence': []})\n",
    "currently_classifying = images.iloc[0,1].rsplit('/')[0]\n",
    "print('Currently: ' + currently_classifying)\n",
    "\n",
    "for _, row in images.iterrows():\n",
    "    if currently_classifying != row['RelativePath'].rsplit('/')[0]:\n",
    "        currently_classifying = row['RelativePath'].rsplit('/')[0]\n",
    "        print('Currently: ' + currently_classifying)\n",
    "    image_path = '../../pictures/' + row['RelativePath'] + '/' + row['File']\n",
    "    cropped_img, category, box, count, humanboxes = detector.bestBoxDetection(image_path)\n",
    "\n",
    "    if cropped_img is not None:\n",
    "        batch = classifier.preprocessImage(cropped_img)\n",
    "        batch = batch.permute(0, 2, 3, 1).contiguous()\n",
    "        preds = classifier.predictOnBatch(batch)\n",
    "        top_idx = np.argmax(preds)\n",
    "        output = txt_animalclasses[top_idx].rsplit(';')[-1]\n",
    "\n",
    "        results.loc[len(results)] = [image_path, output, preds[0][top_idx]]\n",
    "    else:\n",
    "        results.loc[len(results)] = [image_path, 'empty', 0]\n",
    "\n",
    "now = datetime.now().strftime('%Y_%m_%d_%H_%M')\n",
    "results.to_csv('results/results_speciesnet_' + now + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f185bbb",
   "metadata": {},
   "source": [
    "In 5:30 minutes: 1925"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f7bdc1",
   "metadata": {},
   "source": [
    "Total: 175 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d100e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now().strftime('%Y_%m_%d_%H_%M')\n",
    "results.to_csv('results/results_speciesnet_' + now + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af054f7c",
   "metadata": {},
   "source": [
    "##### Series, no YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6967b367",
   "metadata": {},
   "source": [
    "(doesn't work, so probably the detector should be before the classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fefc5c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speciesnet loaded onto cuda\n"
     ]
    }
   ],
   "source": [
    "DFYOLO_NAME = 'deepfaune-yolov8s_960'\n",
    "DFYOLO_PATH = '../deepfaune/models/'\n",
    "DFYOLO_WEIGHTS = DFYOLO_PATH + 'deepfaune-yolov8s_960.pt'\n",
    "SPECIESNET_PATH = 'models/speciesnet-pytorch-v4.0.1a-v1/'\n",
    "SPECIESNET_MODEL = SPECIESNET_PATH + 'always_crop_99710272_22x8_v12_epoch_00148.pt'\n",
    "BACKBONE = 'efficientnetv2_m'  # not neccessary\n",
    "\n",
    "LABELS_FILE = 'models/speciesnet-pytorch-v4.0.1a-v1/always_crop_99710272_22x8_v12_epoch_00148.labels.txt'\n",
    "\n",
    "CROP_SIZE = 480 # default 480??\n",
    "DFYOLO_WIDTH = 960 # image width\n",
    "DFYOLO_THRES = 0.6\n",
    "DFYOLOHUMAN_THRES = 0.4 # boxes with human above this threshold are saved\n",
    "DFYOLOCOUNT_THRES = 0.6\n",
    "\n",
    "\n",
    "with open(LABELS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    txt_animalclasses = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "\n",
    "class Classifier:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = torch.load(SPECIESNET_MODEL, map_location=self.device, weights_only=False)\n",
    "        self.model.eval()\n",
    "        print(f'Speciesnet loaded onto {self.device}')\n",
    "\n",
    "        # transform image to form usable by network\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.Resize(size=(CROP_SIZE, CROP_SIZE), interpolation=InterpolationMode.BICUBIC),\n",
    "            transforms.ToTensor()\n",
    "            # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def predictOnBatch(self, batchtensor, withsoftmax=True):\n",
    "        batchtensor = batchtensor.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(batchtensor)\n",
    "            preds = logits.softmax(dim=1) if withsoftmax else logits\n",
    "        return preds.cpu().numpy()\n",
    "\n",
    "    def preprocessImage(self, image):\n",
    "        return self.transforms(image).unsqueeze(dim=0)  # batch dimension\n",
    "\n",
    "\n",
    "classifier = Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b524b8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = pd.read_csv('../y_clean.csv', index_col=0)\n",
    "images.RelativePath = images.RelativePath.str.replace('\\\\', '/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62e54c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently: 01_CZARNE\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [24, 3, 3, 3], expected input[1, 480, 4, 481] to have 3 channels, but got 480 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m batch = classifier.preprocessImage(img)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# batch = batch.permute(0, 2, 3, 1).contiguous()\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m preds = \u001b[43mclassifier\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredictOnBatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m top_idx = np.argmax(preds)\n\u001b[32m     16\u001b[39m output = txt_animalclasses[top_idx].rsplit(\u001b[33m'\u001b[39m\u001b[33m;\u001b[39m\u001b[33m'\u001b[39m)[-\u001b[32m1\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mClassifier.predictOnBatch\u001b[39m\u001b[34m(self, batchtensor, withsoftmax)\u001b[39m\n\u001b[32m     37\u001b[39m batchtensor = batchtensor.to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatchtensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m     preds = logits.softmax(dim=\u001b[32m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m withsoftmax \u001b[38;5;28;01melse\u001b[39;00m logits\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m preds.cpu().numpy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/plk/TOSHIBA EXT/mgr/.venv/lib/python3.12/site-packages/torch/fx/graph_module.py:837\u001b[39m, in \u001b[36mGraphModule.recompile.<locals>.call_wrapped\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    836\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_wrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m837\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wrapped_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/plk/TOSHIBA EXT/mgr/.venv/lib/python3.12/site-packages/torch/fx/graph_module.py:413\u001b[39m, in \u001b[36m_WrappedCall.__call__\u001b[39m\u001b[34m(self, obj, *args, **kwargs)\u001b[39m\n\u001b[32m    411\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(\u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# noqa: B904\u001b[39;00m\n\u001b[32m    412\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m413\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/plk/TOSHIBA EXT/mgr/.venv/lib/python3.12/site-packages/torch/fx/graph_module.py:400\u001b[39m, in \u001b[36m_WrappedCall.__call__\u001b[39m\u001b[34m(self, obj, *args, **kwargs)\u001b[39m\n\u001b[32m    398\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cls_call(obj, *args, **kwargs)\n\u001b[32m    399\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m e.__traceback__\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/plk/TOSHIBA EXT/mgr/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/plk/TOSHIBA EXT/mgr/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<eval_with_key>.7 from <eval_with_key>.6:10 in forward:10\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(self, input_1)\u001b[39m\n\u001b[32m      8\u001b[39m species_net_efficientnetv2_m_rescaling_add = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mSpeciesNet/efficientnetv2-m/rescaling/add\u001b[39m\u001b[33m\"\u001b[39m)(species_net_efficientnetv2_m_rescaling_mul, initializers_onnx_initializer_1);  species_net_efficientnetv2_m_rescaling_mul = initializers_onnx_initializer_1 = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m      9\u001b[39m species_net_efficientnetv2_m_stem_conv_conv2d__6 = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mSpeciesNet/efficientnetv2-m/stem_conv/Conv2D__6\u001b[39m\u001b[33m\"\u001b[39m)(species_net_efficientnetv2_m_rescaling_add);  species_net_efficientnetv2_m_rescaling_add = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m species_net_efficientnetv2_m_stem_conv_conv2d = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSpeciesNet/efficientnetv2-m/stem_conv/Conv2D\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspecies_net_efficientnetv2_m_stem_conv_conv2d__6\u001b[49m\u001b[43m)\u001b[49m;  species_net_efficientnetv2_m_stem_conv_conv2d__6 = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     11\u001b[39m species_net_efficientnetv2_m_stem_activation_sigmoid = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mSpeciesNet/efficientnetv2-m/stem_activation/Sigmoid\u001b[39m\u001b[33m\"\u001b[39m)(species_net_efficientnetv2_m_stem_conv_conv2d)\n\u001b[32m     12\u001b[39m species_net_efficientnetv2_m_stem_activation_mul_1 = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mSpeciesNet/efficientnetv2-m/stem_activation/mul_1\u001b[39m\u001b[33m\"\u001b[39m)(species_net_efficientnetv2_m_stem_conv_conv2d, species_net_efficientnetv2_m_stem_activation_sigmoid);  species_net_efficientnetv2_m_stem_conv_conv2d = species_net_efficientnetv2_m_stem_activation_sigmoid = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/plk/TOSHIBA EXT/mgr/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/plk/TOSHIBA EXT/mgr/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/plk/TOSHIBA EXT/mgr/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/plk/TOSHIBA EXT/mgr/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/plk/TOSHIBA EXT/mgr/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/plk/TOSHIBA EXT/mgr/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/plk/TOSHIBA EXT/mgr/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:543\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    533\u001b[39m         F.pad(\n\u001b[32m    534\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    542\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Given groups=1, weight of size [24, 3, 3, 3], expected input[1, 480, 4, 481] to have 3 channels, but got 480 channels instead"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame({'image': [], 'detected_animal': [], 'confidence': []})\n",
    "currently_classifying = images.iloc[0,1].rsplit('/')[0]\n",
    "print('Currently: ' + currently_classifying)\n",
    "\n",
    "for _, row in images.iterrows():\n",
    "    if currently_classifying != row['RelativePath'].rsplit('/')[0]:\n",
    "        currently_classifying = row['RelativePath'].rsplit('/')[0]\n",
    "        print('Currently: ' + currently_classifying)\n",
    "    image_path = '../../pictures/' + row['RelativePath'] + '/' + row['File']\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    batch = classifier.preprocessImage(img)\n",
    "    # batch = batch.permute(0, 2, 3, 1).contiguous()\n",
    "    preds = classifier.predictOnBatch(batch)\n",
    "    top_idx = np.argmax(preds)\n",
    "    output = txt_animalclasses[top_idx].rsplit(';')[-1]\n",
    "\n",
    "    results.loc[len(results)] = [image_path, output, preds[0][top_idx]]\n",
    "\n",
    "now = datetime.now().strftime('%Y_%m_%d_%H_%M')\n",
    "results.to_csv('results/results_speciesnet_' + now + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea991579",
   "metadata": {},
   "source": [
    "#### Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4876bdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = pd.read_csv('results/results_speciesnet_2025_11_05_20_10.csv', index_col=0)[:29000]\n",
    "d2 = pd.read_csv('results/results_speciesnet_2025_11_05_22_27.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "484a62f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = pd.concat([d1, d2], ignore_index=True)\n",
    "dc.to_csv('results/results_speciesnet_2025_11_06_7_52.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
