{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "547a3f9e",
   "metadata": {},
   "source": [
    "Classify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4771a3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/plk/TOSHIBA EXT/mgr/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import ast\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from torch import tensor\n",
    "from torchvision.transforms import transforms, InterpolationMode\n",
    "\n",
    "SPECIESNET_PATH = 'models/speciesnet-pytorch-v4.0.1a-v1/'\n",
    "SPECIESNET_MODEL = SPECIESNET_PATH + 'always_crop_99710272_22x8_v12_epoch_00148.pt'\n",
    "\n",
    "LABELS_FILE = 'models/speciesnet-pytorch-v4.0.1a-v1/always_crop_99710272_22x8_v12_epoch_00148.labels.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d787524",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = pd.read_csv('../megadetector_results.csv', index_col=0)\n",
    "images['image_path'] = '../' + images['image_path']\n",
    "images['bbox'] = images[\"bbox\"].apply(\n",
    "    lambda b: ast.literal_eval(b) if isinstance(b, str) else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b46c1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speciesnet loaded onto cuda\n"
     ]
    }
   ],
   "source": [
    "CROP_SIZE = 480\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "with open(LABELS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    classes_labels = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "class Classifier:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = torch.load(SPECIESNET_MODEL, map_location=self.device, weights_only=False)\n",
    "        self.model.eval()\n",
    "        print(f'Speciesnet loaded onto {self.device}')\n",
    "\n",
    "        # transform image to form usable by network\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.Resize(size=(CROP_SIZE, CROP_SIZE), interpolation=InterpolationMode.BICUBIC),\n",
    "            transforms.ToTensor()\n",
    "            # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def predictOnBatch(self, pil_images, withsoftmax=True):\n",
    "        \"\"\"\n",
    "        pil_images: list of PIL.Image\n",
    "        returns: np.ndarray of shape (B, num_classes)\n",
    "        \"\"\"\n",
    "        # Transform all images\n",
    "        tensors = [self.transforms(im) for im in pil_images]   # list of (3, H, W)\n",
    "        batch = torch.stack(tensors).to(self.device)           # (B, 3, H, W)\n",
    "        batch = batch.permute(0, 2, 3, 1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(batch)\n",
    "            preds = logits.softmax(dim=1) if withsoftmax else logits\n",
    "\n",
    "        return preds.cpu()\n",
    "    \n",
    "def crop_normalized_bbox(img: Image.Image, bbox: list[float]):\n",
    "    \"\"\"\n",
    "    img: PIL.Image opened image\n",
    "    bbox: list [x, y, w, h], normalized 0-1\n",
    "    returns cropped PIL.Image\n",
    "    \"\"\"\n",
    "    W, H = img.size\n",
    "    x, y, w, h = bbox\n",
    "\n",
    "    left   = int(x * W)\n",
    "    top    = int(y * H)\n",
    "    right  = int((x + w) * W)\n",
    "    bottom = int((y + h) * H)\n",
    "\n",
    "    return img.crop((left, top, right, bottom))\n",
    "\n",
    "classifier = Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1dd397c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|‚ñè         | 239/18107 [00:27<34:50,  8.55it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m batch.append(cropped_image)\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batch) == BATCH_SIZE:\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     preds = \u001b[43mclassifier\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredictOnBatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     top_probs, class_indxes = preds.max(dim=\u001b[32m1\u001b[39m)\n\u001b[32m     26\u001b[39m     confs = top_probs.cpu().numpy()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mClassifier.predictOnBatch\u001b[39m\u001b[34m(self, pil_images, withsoftmax)\u001b[39m\n\u001b[32m     29\u001b[39m batch = batch.permute(\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m     preds = logits.softmax(dim=\u001b[32m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m withsoftmax \u001b[38;5;28;01melse\u001b[39;00m logits\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m preds.cpu()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/plk/TOSHIBA EXT/mgr/.venv/lib/python3.12/site-packages/torch/fx/graph_module.py:837\u001b[39m, in \u001b[36mGraphModule.recompile.<locals>.call_wrapped\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    836\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_wrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m837\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wrapped_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/plk/TOSHIBA EXT/mgr/.venv/lib/python3.12/site-packages/torch/fx/graph_module.py:400\u001b[39m, in \u001b[36m_WrappedCall.__call__\u001b[39m\u001b[34m(self, obj, *args, **kwargs)\u001b[39m\n\u001b[32m    398\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cls_call(obj, *args, **kwargs)\n\u001b[32m    399\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m e.__traceback__\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/plk/TOSHIBA EXT/mgr/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/plk/TOSHIBA EXT/mgr/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<eval_with_key>.5 from <eval_with_key>.4:10 in forward:81\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(self, input_1)\u001b[39m\n\u001b[32m     79\u001b[39m species_net_efficientnetv2_m_block4a_se_squeeze_mean = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mSpeciesNet/efficientnetv2-m/block4a_se_squeeze/Mean\u001b[39m\u001b[33m\"\u001b[39m)(species_net_efficientnetv2_m_block4a_activation_mul_1)\n\u001b[32m     80\u001b[39m initializers_onnx_initializer_2 = \u001b[38;5;28mself\u001b[39m.initializers.onnx_initializer_2\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m species_net_efficientnetv2_m_block4a_se_squeeze_mean_squeeze__3839 = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSpeciesNet/efficientnetv2-m/block4a_se_squeeze/Mean_Squeeze__3839\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspecies_net_efficientnetv2_m_block4a_se_squeeze_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitializers_onnx_initializer_2\u001b[49m\u001b[43m)\u001b[49m;  species_net_efficientnetv2_m_block4a_se_squeeze_mean = initializers_onnx_initializer_2 = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     82\u001b[39m initializers_onnx_initializer_3 = \u001b[38;5;28mself\u001b[39m.initializers.onnx_initializer_3\n\u001b[32m     83\u001b[39m species_net_efficientnetv2_m_block4a_se_reduce_bias_add__233 = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mSpeciesNet/efficientnetv2-m/block4a_se_reduce/BiasAdd__233\u001b[39m\u001b[33m\"\u001b[39m)(species_net_efficientnetv2_m_block4a_se_squeeze_mean_squeeze__3839, initializers_onnx_initializer_3);  species_net_efficientnetv2_m_block4a_se_squeeze_mean_squeeze__3839 = initializers_onnx_initializer_3 = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/plk/TOSHIBA EXT/mgr/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/plk/TOSHIBA EXT/mgr/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/plk/TOSHIBA EXT/mgr/.venv/lib/python3.12/site-packages/onnx2torch/node_converters/squeeze.py:82\u001b[39m, in \u001b[36mOnnxSqueezeDynamicAxes.forward\u001b[39m\u001b[34m(self, input_tensor, axes)\u001b[39m\n\u001b[32m     78\u001b[39m         args.append(axes)\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DefaultExportToOnnx.export(_forward, \u001b[33m'\u001b[39m\u001b[33mSqueeze\u001b[39m\u001b[33m'\u001b[39m, *args, {})\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/plk/TOSHIBA EXT/mgr/.venv/lib/python3.12/site-packages/onnx2torch/node_converters/squeeze.py:71\u001b[39m, in \u001b[36mOnnxSqueezeDynamicAxes.forward.<locals>._forward\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     69\u001b[39m result = input_tensor\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m axes_id \u001b[38;5;129;01min\u001b[39;00m torch.sort(axes, descending=\u001b[38;5;28;01mTrue\u001b[39;00m).values:\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     result = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxes_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame({'image': [], 'detected_animal': [], 'confidence': []})\n",
    "batch = []\n",
    "paths = []\n",
    "\n",
    "for _, row in tqdm(images.iterrows(), total=len(images)):\n",
    "    image_path = row['image_path']\n",
    "\n",
    "    category = row['category']\n",
    "    if category != 1:\n",
    "        results.loc[len(results)] = [image_path, 'empty', 0]\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        cropped_image = crop_normalized_bbox(image, row['bbox'])\n",
    "    except Exception as e:\n",
    "        print(f'Error in image {image_path}: {e}')\n",
    "        continue\n",
    "\n",
    "    paths.append(image_path)\n",
    "    batch.append(cropped_image)\n",
    "\n",
    "    if len(batch) == BATCH_SIZE:\n",
    "        preds = classifier.predictOnBatch(batch)\n",
    "        top_probs, class_indxes = preds.max(dim=1)\n",
    "        confs = top_probs.cpu().numpy()\n",
    "        detections = [classes_labels[idx].split(';')[-1] for idx in class_indxes]\n",
    "\n",
    "        batch_results = pd.DataFrame(\n",
    "            {'image': paths, 'detected_animal': detections, 'confidence': confs})\n",
    "        results = pd.concat([results, batch_results], ignore_index=True)\n",
    "        batch = []\n",
    "        paths = []\n",
    "\n",
    "if len(batch) > 0:\n",
    "    preds = classifier.predictOnBatch(batch)\n",
    "    top_probs, class_indxes = preds.max(dim=1)\n",
    "    confs = top_probs.cpu().numpy()\n",
    "    detections = [classes_labels[idx] for idx in class_indxes]\n",
    "\n",
    "    batch_results = pd.DataFrame(\n",
    "        {'image': paths, 'detected_animal': detections, 'confidence': confs})\n",
    "    results = pd.concat([results, batch_results], ignore_index=True)\n",
    "\n",
    "now = datetime.now().strftime('%Y_%m_%d_%H_%M')\n",
    "results.to_csv('results/results_speciesnet_' + now + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea991579",
   "metadata": {},
   "source": [
    "#### Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4876bdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = pd.read_csv('results/results_speciesnet_2025_11_05_20_10.csv', index_col=0)[:29000]\n",
    "d2 = pd.read_csv('results/results_speciesnet_2025_11_05_22_27.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "484a62f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = pd.concat([d1, d2], ignore_index=True)\n",
    "dc.to_csv('results/results_speciesnet_2025_11_06_7_52.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
