{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "148a1be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR Error reading from C:\\Users\\plk\\AppData\\Roaming\\Ultralytics\\settings.json: \"No Ultralytics setting 'openvino_msg'. \\nView Ultralytics Settings with 'yolo settings' or at 'C:\\\\Users\\\\plk\\\\AppData\\\\Roaming\\\\Ultralytics\\\\settings.json'\\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\"\n",
      "Creating new Ultralytics Settings v0.0.6 file  \n",
      "View Ultralytics Settings with 'yolo settings' or at 'C:\\Users\\plk\\AppData\\Roaming\\Ultralytics\\settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Pobrane\\mgr\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import tensor\n",
    "from torchvision.transforms import transforms, InterpolationMode\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01fab5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DFPATH = 'models'\n",
    "DFYOLO_NAME = \"deepfaune-yolov8s_960\"\n",
    "BACKBONE = \"vit_large_patch14_dinov2.lvd142m\"\n",
    "DFYOLO_WEIGHTS = os.path.join(DFPATH,'deepfaune-yolov8s_960.pt')\n",
    "DFVIT_WEIGHTS = os.path.join(DFPATH,'deepfaune-vit_large_patch14_dinov2.lvd142m.v3.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd8d28f",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13fe62d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "CROP_SIZE = 182 # default 182, has to be divisible by 14, 280?\n",
    "DFYOLO_WIDTH = 960 # image width\n",
    "DFYOLO_THRES = 0.6\n",
    "DFYOLOHUMAN_THRES = 0.4 # boxes with human above this threshold are saved\n",
    "DFYOLOCOUNT_THRES = 0.6\n",
    "\n",
    "txt_animalclasses = {\n",
    "   'en': ['bison', 'badger', 'ibex', 'beaver', 'red deer', 'chamois', 'cat', 'goat', 'roe deer', 'dog', 'fallow deer', 'squirrel', 'moose', 'equid', 'genet', 'wolverine', 'hedgehog', 'lagomorph', 'wolf', 'otter', 'lynx', 'marmot', 'micromammal', 'mouflon', 'sheep', 'mustelid', 'bird', 'bear', 'nutria', 'raccoon', 'fox', 'reindeer', 'wild boar', 'cow'],\n",
    "   'pl': ['żubr', 'borsuk', 'koziorożec', 'bóbr', 'jeleń szlachetny', 'kozica', 'kot', 'koza', 'sarna', 'pies', 'daniel', 'wiewiórka', 'łoś', 'końowaty', 'żeneta', 'rosomak', 'jeż', 'zajęczak', 'wilk', 'wydra', 'ryś', 'świstak', 'mikrozwierzę', 'muflon', 'owca', 'łasica', 'ptak', 'niedźwiedź', 'nutria', 'szop', 'lis', 'renifer', 'dzik', 'krowa']\n",
    "    #'fr': ['bison', 'blaireau', 'bouquetin', 'castor', 'cerf', 'chamois', 'chat', 'chevre', 'chevreuil', 'chien', 'daim', 'ecureuil', 'elan', 'equide', 'genette', 'glouton', 'herisson', 'lagomorphe', 'loup', 'loutre', 'lynx', 'marmotte', 'micromammifere', 'mouflon', 'mouton', 'mustelide', 'oiseau', 'ours', 'ragondin', 'raton laveur', 'renard', 'renne', 'sanglier', 'vache'],\n",
    "    #'it': ['bisonte', 'tasso', 'stambecco', 'castoro', 'cervo', 'camoscio', 'gatto', 'capra', 'capriolo', 'cane', 'daino', 'scoiattolo', 'alce', 'equide', 'genetta', 'ghiottone', 'riccio', 'lagomorfo', 'lupo', 'lontra', 'lince', 'marmotta', 'micromammifero', 'muflone', 'pecora', 'mustelide', 'uccello', 'orso', 'nutria', 'procione', 'volpe', 'renna', 'cinghiale', 'mucca'],\n",
    "    #'de': ['Bison', 'Dachs', 'Steinbock', 'Biber', 'Rothirsch', 'Gämse', 'Katze', 'Ziege', 'Rehwild', 'Hund', 'Damwild', 'Eichhörnchen', 'Elch', 'Equide', 'Ginsterkatze', 'Vielfraß', 'Igel', 'Lagomorpha', 'Wolf', 'Otter', 'Luchs', 'Murmeltier', 'Kleinsäuger', 'Mufflon', 'Schaf', 'Marder', 'Vogel', 'Bär', 'Nutria', 'Waschbär', 'Fuchs', 'Rentier', 'Wildschwein', 'Kuh'],\n",
    "}\n",
    "\n",
    "class Detector:\n",
    "    def __init__(self, name=DFYOLO_NAME, threshold=None, countthreshold=None, humanthreshold=None):\n",
    "        print(\"Using \"+DFYOLO_NAME+\" with weights at \"+DFYOLO_WEIGHTS+\", in resolution 960x960\")\n",
    "        self.yolo = YOLO(DFYOLO_WEIGHTS)\n",
    "        self.imgsz = DFYOLO_WIDTH\n",
    "        self.threshold = DFYOLO_THRES if threshold is None else threshold\n",
    "        self.countthreshold = DFYOLOCOUNT_THRES if countthreshold is None else countthreshold\n",
    "        self.humanthreshold = DFYOLOHUMAN_THRES if humanthreshold is None else humanthreshold\n",
    "\n",
    "    def bestBoxDetection(self, filename_or_imagecv):\n",
    "        try:\n",
    "            results = self.yolo(filename_or_imagecv, verbose=False, imgsz=self.imgsz)\n",
    "        except FileNotFoundError:\n",
    "            print(\"File not found\")\n",
    "            return None, 0, np.zeros(4), 0, []\n",
    "        except Exception as err:\n",
    "            print(err)\n",
    "            return None, 0, np.zeros(4), 0, []\n",
    "\n",
    "        # orig_img a numpy array (cv2) in BGR\n",
    "        imagecv = results[0].cpu().orig_img\n",
    "        detection = results[0].cpu().numpy().boxes\n",
    "\n",
    "        # Are there any relevant boxes?\n",
    "        if not len(detection.cls) or detection.conf[0] < self.threshold:\n",
    "            # No. Image considered as empty\n",
    "            return None, 0, np.zeros(4), 0, []\n",
    "        else:\n",
    "            # Yes. Non empty image\n",
    "            pass\n",
    "        # Is there a relevant animal box? \n",
    "        try:\n",
    "            # Yes. Selecting the best animal box\n",
    "            kbox = np.where((detection.cls==0) & (detection.conf>self.threshold))[0][0]\n",
    "        except IndexError:\n",
    "            # No: Selecting the best box for another category (human, vehicle)\n",
    "            kbox = 0\n",
    "\n",
    "        # categories are 1=animal, 2=person, 3=vehicle and the empty category 0=empty\n",
    "        category = int(detection.cls[kbox]) + 1\n",
    "        box = detection.xyxy[kbox] # xmin, ymin, xmax, ymax\n",
    "\n",
    "        # Is this an animal box ?\n",
    "        if category == 1:\n",
    "            # Yes: cropped image is required for classification\n",
    "            croppedimage = cropSquareCVtoPIL(imagecv, box.copy())\n",
    "        else: \n",
    "            # No: return none\n",
    "            return None, 0, np.zeros(4), 0, []\n",
    "        \n",
    "        ## animal count\n",
    "        if category == 1:\n",
    "            count = sum((detection.conf>self.countthreshold) & (detection.cls==0)) # only above a threshold\n",
    "        else:\n",
    "            count = 0\n",
    "        ## human boxes\n",
    "        ishuman = (detection.cls==1) & (detection.conf>=self.humanthreshold)\n",
    "        if any(ishuman==True):\n",
    "            humanboxes = detection.xyxy[ishuman,]\n",
    "        else:\n",
    "            humanboxes = []\n",
    "\n",
    "        return croppedimage, category, box, count, humanboxes\n",
    "\n",
    "\n",
    "def cropSquareCVtoPIL(imagecv, box):\n",
    "    x1, y1, x2, y2 = box\n",
    "    xsize = (x2-x1)\n",
    "    ysize = (y2-y1)\n",
    "    if xsize>ysize:\n",
    "        y1 = y1-int((xsize-ysize)/2)\n",
    "        y2 = y2+int((xsize-ysize)/2)\n",
    "    if ysize>xsize:\n",
    "        x1 = x1-int((ysize-xsize)/2)\n",
    "        x2 = x2+int((ysize-xsize)/2)\n",
    "    height, width, _ = imagecv.shape\n",
    "    croppedimagecv = imagecv[max(0,int(y1)):min(int(y2),height),max(0,int(x1)):min(int(x2),width)]\n",
    "    croppedimage = Image.fromarray(croppedimagecv[:,:,(2,1,0)]) # converted to PIL BGR image\n",
    "    return croppedimage\n",
    "\n",
    "\n",
    "class Classifier:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = Model()\n",
    "        self.model.loadWeights(DFVIT_WEIGHTS)\n",
    "        # transform image to form usable by network\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.Resize(size=(CROP_SIZE, CROP_SIZE), interpolation=InterpolationMode.BICUBIC, max_size=None, antialias=None),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))])\n",
    "\n",
    "    def predictOnBatch(self, batchtensor, withsoftmax=True):\n",
    "        return self.model.predict(batchtensor, withsoftmax)\n",
    "\n",
    "    # croppedimage loaded by PIL\n",
    "    def preprocessImage(self, croppedimage):\n",
    "        preprocessimage = self.transforms(croppedimage)\n",
    "        return preprocessimage.unsqueeze(dim=0)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor of model classifier\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.base_model = timm.create_model(BACKBONE, pretrained=False, num_classes=len(txt_animalclasses['en']),\n",
    "                                            dynamic_img_size=True)\n",
    "        print(f\"Using {BACKBONE} with weights at {DFVIT_WEIGHTS}, in resolution {CROP_SIZE}x{CROP_SIZE}\")\n",
    "        self.backbone = BACKBONE\n",
    "        self.nbclasses = len(txt_animalclasses['en'])\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.base_model(input)\n",
    "        return x\n",
    "\n",
    "    def predict(self, data, withsoftmax=True):\n",
    "        \"\"\"\n",
    "        Predict on test DataLoader\n",
    "        :param test_loader: test dataloader: torch.utils.data.DataLoader\n",
    "        :return: numpy array of predictions without soft max\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(device)\n",
    "        total_output = []\n",
    "        with torch.no_grad():\n",
    "            x = data.to(device)\n",
    "            if withsoftmax:\n",
    "                output = self.forward(x).softmax(dim=1)\n",
    "            else:\n",
    "                output = self.forward(x)\n",
    "            total_output += output.tolist()\n",
    "\n",
    "        return np.array(total_output)\n",
    "\n",
    "    def loadWeights(self, path):\n",
    "        \"\"\"\n",
    "        :param path: path of .pt save of model\n",
    "        \"\"\"\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(\"CUDA available\" if torch.cuda.is_available() else \"CUDA unavailable. Using CPU\")\n",
    "\n",
    "        try:\n",
    "            params = torch.load(path, map_location=device)\n",
    "            args = params['args']\n",
    "            if self.nbclasses != args['num_classes']:\n",
    "                raise Exception(\"You load a model ({}) that does not have the same number of class\"\n",
    "                                \"({})\".format(args['num_classes'], self.nbclasses))\n",
    "            self.backbone = args['backbone']\n",
    "            self.nbclasses = args['num_classes']\n",
    "            self.load_state_dict(params['state_dict'])\n",
    "        except Exception as e:\n",
    "            print(\"Can't load checkpoint model because :\\n\\n \" + str(e), file=sys.stderr)\n",
    "            raise e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea321244",
   "metadata": {},
   "source": [
    "#### Detect animal and crop image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c9272c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using deepfaune-yolov8s_960 with weights at models\\deepfaune-yolov8s_960.pt, in resolution 960x960\n"
     ]
    }
   ],
   "source": [
    "IMAGES_PATH = 'test_images/'\n",
    "IMAGE_FILE = '2022-09-06 19-34-11.JPG'\n",
    "\n",
    "det = Detector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6a8dfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "croppedimage, category, box, count, humanboxes = det.bestBoxDetection(IMAGES_PATH + IMAGE_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7591409",
   "metadata": {},
   "source": [
    "#### Classify animal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bab173a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vit_large_patch14_dinov2.lvd142m with weights at models\\deepfaune-vit_large_patch14_dinov2.lvd142m.v3.pt, in resolution 182x182\n",
      "CUDA available\n"
     ]
    }
   ],
   "source": [
    "classifier = Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16bf1660",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unexpected type <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m cropped_tensor = torch.ones((\u001b[32m1\u001b[39m,\u001b[32m3\u001b[39m,CROP_SIZE,CROP_SIZE))\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m cropped_tensor[\u001b[32m0\u001b[39m,:,:,:] =  \u001b[43mclassifier\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpreprocessImage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcroppedimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m scores = classifier.model.predict(cropped_tensor)\n\u001b[32m      4\u001b[39m scores\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 112\u001b[39m, in \u001b[36mClassifier.preprocessImage\u001b[39m\u001b[34m(self, croppedimage)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpreprocessImage\u001b[39m(\u001b[38;5;28mself\u001b[39m, croppedimage):\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     preprocessimage = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransforms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcroppedimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m preprocessimage.unsqueeze(dim=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Pobrane\\mgr\\.venv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Pobrane\\mgr\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Pobrane\\mgr\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Pobrane\\mgr\\.venv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:354\u001b[39m, in \u001b[36mResize.forward\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m    347\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    348\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    349\u001b[39m \u001b[33;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    352\u001b[39m \u001b[33;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Pobrane\\mgr\\.venv\\Lib\\site-packages\\torchvision\\transforms\\functional.py:465\u001b[39m, in \u001b[36mresize\u001b[39m\u001b[34m(img, size, interpolation, max_size, antialias)\u001b[39m\n\u001b[32m    459\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m max_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) != \u001b[32m1\u001b[39m:\n\u001b[32m    460\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    461\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmax_size should only be passed if size specifies the length of the smaller edge, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    462\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mi.e. size should be an int or a sequence of length 1 in torchscript mode.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    463\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m _, image_height, image_width = \u001b[43mget_dimensions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mint\u001b[39m):\n\u001b[32m    467\u001b[39m     size = [size]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Pobrane\\mgr\\.venv\\Lib\\site-packages\\torchvision\\transforms\\functional.py:80\u001b[39m, in \u001b[36mget_dimensions\u001b[39m\u001b[34m(img)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch.Tensor):\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F_t.get_dimensions(img)\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_dimensions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Pobrane\\mgr\\.venv\\Lib\\site-packages\\torchvision\\transforms\\_functional_pil.py:31\u001b[39m, in \u001b[36mget_dimensions\u001b[39m\u001b[34m(img)\u001b[39m\n\u001b[32m     29\u001b[39m     width, height = img.size\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [channels, height, width]\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnexpected type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(img)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: Unexpected type <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "cropped_tensor = torch.ones((1,3,CROP_SIZE,CROP_SIZE))\n",
    "cropped_tensor[0,:,:,:] =  classifier.preprocessImage(croppedimage)\n",
    "scores = classifier.model.predict(cropped_tensor)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3627ec79",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'species':txt_animalclasses['pl'], 'score':scores[0][:]})\n",
    "output = output[output['score'] > 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03bf0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Wykryto zwierzę: {output.loc[output['score'].idxmax(), 'species']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d536613f",
   "metadata": {},
   "outputs": [],
   "source": [
    "croppedimage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb0ac12",
   "metadata": {},
   "source": [
    "#### Pipeline klasyfikacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d616468",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = Detector()\n",
    "classifier = Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d720177",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH = 'test_images/'\n",
    "IMAGE_FILE = '2023-05-01 00-04-04.JPG'\n",
    "\n",
    "croppedimage, category, box, count, humanboxes = detector.bestBoxDetection(IMAGES_PATH + IMAGE_FILE)\n",
    "if croppedimage is not None:\n",
    "    cropped_tensor = torch.ones((1,3,CROP_SIZE,CROP_SIZE))\n",
    "    cropped_tensor[0,:,:,:] =  classifier.preprocessImage(croppedimage)\n",
    "    scores = classifier.model.predict(cropped_tensor)\n",
    "    output = pd.DataFrame({'species':txt_animalclasses['pl'], 'score':scores[0][:]})\n",
    "    output = output[output['score'] > 0.2]\n",
    "    print(output)\n",
    "\n",
    "    print(f\"\\nWykryto zwierzę: {output.loc[output['score'].idxmax(), 'species']}\")\n",
    "    display(croppedimage)\n",
    "else:\n",
    "    print('Nie wykryto zwierzęcia')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71f8175",
   "metadata": {},
   "source": [
    "#### Pętla klasyfikacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcdb2f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using deepfaune-yolov8s_960 with weights at models\\deepfaune-yolov8s_960.pt, in resolution 960x960\n",
      "Using vit_large_patch14_dinov2.lvd142m with weights at models\\deepfaune-vit_large_patch14_dinov2.lvd142m.v3.pt, in resolution 182x182\n",
      "CUDA available\n"
     ]
    }
   ],
   "source": [
    "detector = Detector()\n",
    "classifier = Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49a40de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************************************************************************************************\r"
     ]
    }
   ],
   "source": [
    "IMAGES_PATH = 'test_images/'\n",
    "images_names = os.listdir(IMAGES_PATH)\n",
    "\n",
    "print_list = ['-' for i in images_names]\n",
    "ith_char = 0\n",
    "print(''.join(print_list), end='\\r')\n",
    "\n",
    "results = pd.DataFrame({'image': [], 'detected_animal': []})\n",
    "\n",
    "for img_name in images_names:\n",
    "    croppedimage, category, box, count, humanboxes = detector.bestBoxDetection(IMAGES_PATH + img_name)\n",
    "\n",
    "    if croppedimage is not None:\n",
    "        cropped_tensor = torch.ones((1,3,CROP_SIZE,CROP_SIZE))\n",
    "        cropped_tensor[0,:,:,:] =  classifier.preprocessImage(croppedimage)\n",
    "        scores = classifier.model.predict(cropped_tensor)\n",
    "        output = pd.DataFrame({'species':txt_animalclasses['pl'], 'score':scores[0][:]})\n",
    "        output = output[output['score'] > 0.2]\n",
    "\n",
    "        results.loc[len(results)] = [img_name, output.loc[output['score'].idxmax(), 'species']]\n",
    "    else:\n",
    "        results.loc[len(results)] = [img_name, 'Brak']\n",
    "    \n",
    "    print_list[ith_char] = '*'\n",
    "    print(''.join(print_list), end='\\r')\n",
    "    ith_char += 1\n",
    "\n",
    "results.to_csv('results/results_' + DFYOLO_NAME + '_' + BACKBONE + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc7a2d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>detected_animal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-07 00-47-51.JPG</td>\n",
       "      <td>jeleń szlachetny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-09-05 13-57-18.JPG</td>\n",
       "      <td>lis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-09-06 19-34-11.JPG</td>\n",
       "      <td>Brak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-09-10 22-41-29.JPG</td>\n",
       "      <td>borsuk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-09-13 01-29-08.JPG</td>\n",
       "      <td>Brak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>IMG_0607.JPG</td>\n",
       "      <td>żubr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>IMG_0726.JPG</td>\n",
       "      <td>wilk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>IMG_0800.JPG</td>\n",
       "      <td>kot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>IMG_0814.JPG</td>\n",
       "      <td>lis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>IMG_0894.JPG</td>\n",
       "      <td>jeleń szlachetny</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>121 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image   detected_animal\n",
       "0    2020-01-07 00-47-51.JPG  jeleń szlachetny\n",
       "1    2022-09-05 13-57-18.JPG               lis\n",
       "2    2022-09-06 19-34-11.JPG              Brak\n",
       "3    2022-09-10 22-41-29.JPG            borsuk\n",
       "4    2022-09-13 01-29-08.JPG              Brak\n",
       "..                       ...               ...\n",
       "116             IMG_0607.JPG              żubr\n",
       "117             IMG_0726.JPG              wilk\n",
       "118             IMG_0800.JPG               kot\n",
       "119             IMG_0814.JPG               lis\n",
       "120             IMG_0894.JPG  jeleń szlachetny\n",
       "\n",
       "[121 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dafb1e3",
   "metadata": {},
   "source": [
    "#### Sprawdzanie crop_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135789bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df182 = pd.read_csv('results/results_deepfaune-yolov8s_960_vit_large_patch14_dinov2.lvd142m_cropsize_182.csv')\n",
    "df280 = pd.read_csv('results/results_deepfaune-yolov8s_960_vit_large_patch14_dinov2.lvd142m_cropsize_280.csv')\n",
    "df560 = pd.read_csv('results/results_deepfaune-yolov8s_960_vit_large_patch14_dinov2.lvd142m_cropsize_560.csv')\n",
    "df182.rename(columns={'detected_animal': 'detected_animal_182'}, inplace=True)\n",
    "df280.rename(columns={'detected_animal': 'detected_animal_280'}, inplace=True)\n",
    "df560.rename(columns={'detected_animal': 'detected_animal_560'}, inplace=True)\n",
    "\n",
    "joined = pd.concat([df182, df280, df560], axis=1)[['image', 'detected_animal_182', 'detected_animal_280', 'detected_animal_560']]\n",
    "joined['match'] = ((joined['detected_animal_182'] == joined['detected_animal_280']) \\\n",
    "                   & (joined['detected_animal_280'] == joined['detected_animal_560']) \\\n",
    "                   & (joined['detected_animal_182'] == joined['detected_animal_560']))\n",
    "\n",
    "joined.loc[joined['match'] == False]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
