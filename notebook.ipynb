{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "148a1be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR Error reading from C:\\Users\\plk\\AppData\\Roaming\\Ultralytics\\settings.json: \"No Ultralytics setting 'openvino_msg'. \\nView Ultralytics Settings with 'yolo settings' or at 'C:\\\\Users\\\\plk\\\\AppData\\\\Roaming\\\\Ultralytics\\\\settings.json'\\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\"\n",
      "Creating new Ultralytics Settings v0.0.6 file  \n",
      "View Ultralytics Settings with 'yolo settings' or at 'C:\\Users\\plk\\AppData\\Roaming\\Ultralytics\\settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Pobrane\\mgr\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import tensor\n",
    "from torchvision.transforms import transforms, InterpolationMode\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01fab5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DFPATH = 'models'\n",
    "DFYOLO_NAME = \"deepfaune-yolov8s_960\"\n",
    "BACKBONE = \"vit_large_patch14_dinov2.lvd142m\"\n",
    "DFYOLO_WEIGHTS = os.path.join(DFPATH,'deepfaune-yolov8s_960.pt')\n",
    "DFVIT_WEIGHTS = os.path.join(DFPATH,'deepfaune-vit_large_patch14_dinov2.lvd142m.v3.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd8d28f",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13fe62d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "CROP_SIZE = 182 # default 182, has to be divisible by 14, 280?\n",
    "DFYOLO_WIDTH = 960 # image width\n",
    "DFYOLO_THRES = 0.6\n",
    "DFYOLOHUMAN_THRES = 0.4 # boxes with human above this threshold are saved\n",
    "DFYOLOCOUNT_THRES = 0.6\n",
    "\n",
    "txt_animalclasses = {\n",
    "   'en': ['bison', 'badger', 'ibex', 'beaver', 'red deer', 'chamois', 'cat', 'goat', 'roe deer', 'dog', 'fallow deer', 'squirrel', 'moose', 'equid', 'genet', 'wolverine', 'hedgehog', 'lagomorph', 'wolf', 'otter', 'lynx', 'marmot', 'micromammal', 'mouflon', 'sheep', 'mustelid', 'bird', 'bear', 'nutria', 'raccoon', 'fox', 'reindeer', 'wild boar', 'cow'],\n",
    "   'pl': ['bizon', 'borsuk', 'koziorożec', 'bóbr', 'jeleń szlachetny', 'kozica', 'kot', 'koza', 'sarna', 'pies', 'daniel', 'wiewiórka', 'łoś', 'końowaty', 'żeneta', 'rosomak', 'jeż', 'zajęczak', 'wilk', 'wydra', 'ryś', 'świstak', 'mikrozwierzę', 'muflon', 'owca', 'łasica', 'ptak', 'niedźwiedź', 'nutria', 'szop', 'lis', 'renifer', 'dzik', 'krowa']\n",
    "    #'fr': ['bison', 'blaireau', 'bouquetin', 'castor', 'cerf', 'chamois', 'chat', 'chevre', 'chevreuil', 'chien', 'daim', 'ecureuil', 'elan', 'equide', 'genette', 'glouton', 'herisson', 'lagomorphe', 'loup', 'loutre', 'lynx', 'marmotte', 'micromammifere', 'mouflon', 'mouton', 'mustelide', 'oiseau', 'ours', 'ragondin', 'raton laveur', 'renard', 'renne', 'sanglier', 'vache'],\n",
    "    #'it': ['bisonte', 'tasso', 'stambecco', 'castoro', 'cervo', 'camoscio', 'gatto', 'capra', 'capriolo', 'cane', 'daino', 'scoiattolo', 'alce', 'equide', 'genetta', 'ghiottone', 'riccio', 'lagomorfo', 'lupo', 'lontra', 'lince', 'marmotta', 'micromammifero', 'muflone', 'pecora', 'mustelide', 'uccello', 'orso', 'nutria', 'procione', 'volpe', 'renna', 'cinghiale', 'mucca'],\n",
    "    #'de': ['Bison', 'Dachs', 'Steinbock', 'Biber', 'Rothirsch', 'Gämse', 'Katze', 'Ziege', 'Rehwild', 'Hund', 'Damwild', 'Eichhörnchen', 'Elch', 'Equide', 'Ginsterkatze', 'Vielfraß', 'Igel', 'Lagomorpha', 'Wolf', 'Otter', 'Luchs', 'Murmeltier', 'Kleinsäuger', 'Mufflon', 'Schaf', 'Marder', 'Vogel', 'Bär', 'Nutria', 'Waschbär', 'Fuchs', 'Rentier', 'Wildschwein', 'Kuh'],\n",
    "}\n",
    "\n",
    "class Detector:\n",
    "    def __init__(self, name=DFYOLO_NAME, threshold=None, countthreshold=None, humanthreshold=None):\n",
    "        print(\"Using \"+DFYOLO_NAME+\" with weights at \"+DFYOLO_WEIGHTS+\", in resolution 960x960\")\n",
    "        self.yolo = YOLO(DFYOLO_WEIGHTS)\n",
    "        self.imgsz = DFYOLO_WIDTH\n",
    "        self.threshold = DFYOLO_THRES if threshold is None else threshold\n",
    "        self.countthreshold = DFYOLOCOUNT_THRES if countthreshold is None else countthreshold\n",
    "        self.humanthreshold = DFYOLOHUMAN_THRES if humanthreshold is None else humanthreshold\n",
    "\n",
    "    def bestBoxDetection(self, filename_or_imagecv):\n",
    "        try:\n",
    "            results = self.yolo(filename_or_imagecv, verbose=False, imgsz=self.imgsz)\n",
    "        except FileNotFoundError:\n",
    "            print(\"File not found\")\n",
    "            return None, 0, np.zeros(4), 0, []\n",
    "        except Exception as err:\n",
    "            print(err)\n",
    "            return None, 0, np.zeros(4), 0, []\n",
    "\n",
    "        # orig_img a numpy array (cv2) in BGR\n",
    "        imagecv = results[0].cpu().orig_img\n",
    "        detection = results[0].cpu().numpy().boxes\n",
    "\n",
    "        # Are there any relevant boxes?\n",
    "        if not len(detection.cls) or detection.conf[0] < self.threshold:\n",
    "            # No. Image considered as empty\n",
    "            return None, 0, np.zeros(4), 0, []\n",
    "        else:\n",
    "            # Yes. Non empty image\n",
    "            pass\n",
    "        # Is there a relevant animal box? \n",
    "        try:\n",
    "            # Yes. Selecting the best animal box\n",
    "            kbox = np.where((detection.cls==0) & (detection.conf>self.threshold))[0][0]\n",
    "        except IndexError:\n",
    "            # No: Selecting the best box for another category (human, vehicle)\n",
    "            kbox = 0\n",
    "\n",
    "        # categories are 1=animal, 2=person, 3=vehicle and the empty category 0=empty\n",
    "        category = int(detection.cls[kbox]) + 1\n",
    "        box = detection.xyxy[kbox] # xmin, ymin, xmax, ymax\n",
    "\n",
    "        # Is this an animal box ?\n",
    "        if category == 1:\n",
    "            # Yes: cropped image is required for classification\n",
    "            croppedimage = cropSquareCVtoPIL(imagecv, box.copy())\n",
    "        else: \n",
    "            # No: return none\n",
    "            return None, 0, np.zeros(4), 0, []\n",
    "        \n",
    "        ## animal count\n",
    "        if category == 1:\n",
    "            count = sum((detection.conf>self.countthreshold) & (detection.cls==0)) # only above a threshold\n",
    "        else:\n",
    "            count = 0\n",
    "        ## human boxes\n",
    "        ishuman = (detection.cls==1) & (detection.conf>=self.humanthreshold)\n",
    "        if any(ishuman==True):\n",
    "            humanboxes = detection.xyxy[ishuman,]\n",
    "        else:\n",
    "            humanboxes = []\n",
    "\n",
    "        return croppedimage, category, box, count, humanboxes\n",
    "\n",
    "\n",
    "def cropSquareCVtoPIL(imagecv, box):\n",
    "    x1, y1, x2, y2 = box\n",
    "    xsize = (x2-x1)\n",
    "    ysize = (y2-y1)\n",
    "    if xsize>ysize:\n",
    "        y1 = y1-int((xsize-ysize)/2)\n",
    "        y2 = y2+int((xsize-ysize)/2)\n",
    "    if ysize>xsize:\n",
    "        x1 = x1-int((ysize-xsize)/2)\n",
    "        x2 = x2+int((ysize-xsize)/2)\n",
    "    height, width, _ = imagecv.shape\n",
    "    croppedimagecv = imagecv[max(0,int(y1)):min(int(y2),height),max(0,int(x1)):min(int(x2),width)]\n",
    "    croppedimage = Image.fromarray(croppedimagecv[:,:,(2,1,0)]) # converted to PIL BGR image\n",
    "    return croppedimage\n",
    "\n",
    "\n",
    "class Classifier:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = Model()\n",
    "        self.model.loadWeights(DFVIT_WEIGHTS)\n",
    "        # transform image to form usable by network\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.Resize(size=(CROP_SIZE, CROP_SIZE), interpolation=InterpolationMode.BICUBIC, max_size=None, antialias=None),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))])\n",
    "\n",
    "    def predictOnBatch(self, batchtensor, withsoftmax=True):\n",
    "        return self.model.predict(batchtensor, withsoftmax)\n",
    "\n",
    "    # croppedimage loaded by PIL\n",
    "    def preprocessImage(self, croppedimage):\n",
    "        preprocessimage = self.transforms(croppedimage)\n",
    "        return preprocessimage.unsqueeze(dim=0)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor of model classifier\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.base_model = timm.create_model(BACKBONE, pretrained=False, num_classes=len(txt_animalclasses['en']),\n",
    "                                            dynamic_img_size=True)\n",
    "        print(f\"Using {BACKBONE} with weights at {DFVIT_WEIGHTS}, in resolution {CROP_SIZE}x{CROP_SIZE}\")\n",
    "        self.backbone = BACKBONE\n",
    "        self.nbclasses = len(txt_animalclasses['en'])\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.base_model(input)\n",
    "        return x\n",
    "\n",
    "    def predict(self, data, withsoftmax=True):\n",
    "        \"\"\"\n",
    "        Predict on test DataLoader\n",
    "        :param test_loader: test dataloader: torch.utils.data.DataLoader\n",
    "        :return: numpy array of predictions without soft max\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(device)\n",
    "        total_output = []\n",
    "        with torch.no_grad():\n",
    "            x = data.to(device)\n",
    "            if withsoftmax:\n",
    "                output = self.forward(x).softmax(dim=1)\n",
    "            else:\n",
    "                output = self.forward(x)\n",
    "            total_output += output.tolist()\n",
    "\n",
    "        return np.array(total_output)\n",
    "\n",
    "    def loadWeights(self, path):\n",
    "        \"\"\"\n",
    "        :param path: path of .pt save of model\n",
    "        \"\"\"\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(\"CUDA available\" if torch.cuda.is_available() else \"CUDA unavailable. Using CPU\")\n",
    "\n",
    "        try:\n",
    "            params = torch.load(path, map_location=device)\n",
    "            args = params['args']\n",
    "            if self.nbclasses != args['num_classes']:\n",
    "                raise Exception(\"You load a model ({}) that does not have the same number of class\"\n",
    "                                \"({})\".format(args['num_classes'], self.nbclasses))\n",
    "            self.backbone = args['backbone']\n",
    "            self.nbclasses = args['num_classes']\n",
    "            self.load_state_dict(params['state_dict'])\n",
    "        except Exception as e:\n",
    "            print(\"Can't load checkpoint model because :\\n\\n \" + str(e), file=sys.stderr)\n",
    "            raise e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea321244",
   "metadata": {},
   "source": [
    "#### Detect animal and crop image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9272c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH = 'test_images/'\n",
    "IMAGE_FILE = 'IMG_0036.JPG'\n",
    "\n",
    "det = Detector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a8dfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "croppedimage, category, box, count, humanboxes = det.bestBoxDetection(IMAGES_PATH + IMAGE_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7591409",
   "metadata": {},
   "source": [
    "#### Classify animal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bab173a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bf1660",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_tensor = torch.ones((1,3,CROP_SIZE,CROP_SIZE))\n",
    "cropped_tensor[0,:,:,:] =  classifier.preprocessImage(croppedimage)\n",
    "scores = classifier.model.predict(cropped_tensor)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3627ec79",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'species':txt_animalclasses['pl'], 'score':scores[0][:]})\n",
    "output = output[output['score'] > 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03bf0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Wykryto zwierzę: {output.loc[output['score'].idxmax(), 'species']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d536613f",
   "metadata": {},
   "outputs": [],
   "source": [
    "croppedimage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb0ac12",
   "metadata": {},
   "source": [
    "#### Pipeline klasyfikacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d616468",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = Detector()\n",
    "classifier = Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d720177",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH = 'test_images/'\n",
    "IMAGE_FILE = '2023-05-01 00-04-04.JPG'\n",
    "\n",
    "croppedimage, category, box, count, humanboxes = detector.bestBoxDetection(IMAGES_PATH + IMAGE_FILE)\n",
    "if croppedimage is not None:\n",
    "    cropped_tensor = torch.ones((1,3,CROP_SIZE,CROP_SIZE))\n",
    "    cropped_tensor[0,:,:,:] =  classifier.preprocessImage(croppedimage)\n",
    "    scores = classifier.model.predict(cropped_tensor)\n",
    "    output = pd.DataFrame({'species':txt_animalclasses['pl'], 'score':scores[0][:]})\n",
    "    output = output[output['score'] > 0.2]\n",
    "    print(output)\n",
    "\n",
    "    print(f\"\\nWykryto zwierzę: {output.loc[output['score'].idxmax(), 'species']}\")\n",
    "    display(croppedimage)\n",
    "else:\n",
    "    print('Nie wykryto zwierzęcia')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71f8175",
   "metadata": {},
   "source": [
    "#### Pętla klasyfikacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdb2f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = Detector()\n",
    "classifier = Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a40de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH = 'test_images/'\n",
    "images_names = os.listdir(IMAGES_PATH)\n",
    "\n",
    "print_list = ['-' for i in images_names]\n",
    "ith_char = 0\n",
    "print(''.join(print_list), end='\\r')\n",
    "\n",
    "results = pd.DataFrame({'image': [], 'detected_animal': []})\n",
    "\n",
    "for img_name in images_names:\n",
    "    croppedimage, category, box, count, humanboxes = detector.bestBoxDetection(IMAGES_PATH + img_name)\n",
    "\n",
    "    if croppedimage is not None:\n",
    "        cropped_tensor = torch.ones((1,3,CROP_SIZE,CROP_SIZE))\n",
    "        cropped_tensor[0,:,:,:] =  classifier.preprocessImage(croppedimage)\n",
    "        scores = classifier.model.predict(cropped_tensor)\n",
    "        output = pd.DataFrame({'species':txt_animalclasses['pl'], 'score':scores[0][:]})\n",
    "        output = output[output['score'] > 0.2]\n",
    "\n",
    "        results.loc[len(results)] = [img_name, output.loc[output['score'].idxmax(), 'species']]\n",
    "    else:\n",
    "        results.loc[len(results)] = [img_name, 'Brak']\n",
    "    \n",
    "    print_list[ith_char] = '*'\n",
    "    print(''.join(print_list), end='\\r')\n",
    "    ith_char += 1\n",
    "\n",
    "results.to_csv('results/results_' + DFYOLO_NAME + '_' + BACKBONE + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7a2d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dafb1e3",
   "metadata": {},
   "source": [
    "#### Sprawdzanie crop_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135789bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df182 = pd.read_csv('results/results_deepfaune-yolov8s_960_vit_large_patch14_dinov2.lvd142m_cropsize_182.csv')\n",
    "df280 = pd.read_csv('results/results_deepfaune-yolov8s_960_vit_large_patch14_dinov2.lvd142m_cropsize_280.csv')\n",
    "df560 = pd.read_csv('results/results_deepfaune-yolov8s_960_vit_large_patch14_dinov2.lvd142m_cropsize_560.csv')\n",
    "df182.rename(columns={'detected_animal': 'detected_animal_182'}, inplace=True)\n",
    "df280.rename(columns={'detected_animal': 'detected_animal_280'}, inplace=True)\n",
    "df560.rename(columns={'detected_animal': 'detected_animal_560'}, inplace=True)\n",
    "\n",
    "joined = pd.concat([df182, df280, df560], axis=1)[['image', 'detected_animal_182', 'detected_animal_280', 'detected_animal_560']]\n",
    "joined['match'] = ((joined['detected_animal_182'] == joined['detected_animal_280']) \\\n",
    "                   & (joined['detected_animal_280'] == joined['detected_animal_560']) \\\n",
    "                   & (joined['detected_animal_182'] == joined['detected_animal_560']))\n",
    "\n",
    "joined.loc[joined['match'] == False]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
