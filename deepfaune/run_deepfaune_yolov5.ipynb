{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b05339c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import timm\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "387f0161",
   "metadata": {},
   "outputs": [],
   "source": [
    "animals = ['bison', 'badger', 'ibex', 'beaver', 'red deer', 'golden jackal', 'chamois', 'cat', 'goat',\n",
    "           'roe deer', 'dog', 'raccoon dog', 'fallow deer', 'squirrel', 'moose', 'equid', 'genet',\n",
    "           'wolverine', 'hedgehog', 'lagomorph', 'wolf', 'otter', 'lynx', 'marmot', 'micromammal', \n",
    "           'mouflon', 'sheep', 'mustelid', 'bird', 'bear', 'porcupine', 'nutria', 'muskrat', 'raccoon',\n",
    "           'fox', 'reindeer', 'wild boar', 'cow']\n",
    "\n",
    "DFYOLO_NAME = \"DF\"\n",
    "DFYOLO_WIDTH = 960 # image width\n",
    "DFYOLO_THRES = 0.6\n",
    "DFYOLO_WEIGHTS = os.path.join('models/deepfaune-yolov8s_960.pt')\n",
    "\n",
    "CROP_SIZE = 182\n",
    "BACKBONE = \"vit_large_patch14_dinov2.lvd142m\"\n",
    "DFVIT_WEIGHTS = os.path.join('models/deepfaune-vit_large_patch14_dinov2.lvd142m.v4.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e0788be",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv('../y_clean_thin.csv', index_col=0)\n",
    "results.image_path = '../' + results.image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f291e703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cropSquareCVtoPIL(imagecv, box):\n",
    "    x1, y1, x2, y2 = box\n",
    "    xsize = (x2-x1)\n",
    "    ysize = (y2-y1)\n",
    "    if xsize>ysize:\n",
    "        y1 = y1-int((xsize-ysize)/2)\n",
    "        y2 = y2+int((xsize-ysize)/2)\n",
    "    if ysize>xsize:\n",
    "        x1 = x1-int((ysize-xsize)/2)\n",
    "        x2 = x2+int((ysize-xsize)/2)\n",
    "    height, width, _ = imagecv.shape\n",
    "    croppedimagecv = imagecv[max(0,int(y1)):min(int(y2),height),max(0,int(x1)):min(int(x2),width)]\n",
    "    croppedimage = Image.fromarray(croppedimagecv[:,:,(2,1,0)]) # converted to PIL BGR image\n",
    "    return croppedimage\n",
    "\n",
    "\n",
    "class Detector:\n",
    "    def __init__(self, device: str='cuda'):\n",
    "        self.device = device\n",
    "        self.yolo = YOLO(DFYOLO_WEIGHTS)\n",
    "\n",
    "    def bestBoxDetection(self, filename_or_imagecv):\n",
    "        try:\n",
    "            results = self.yolo(filename_or_imagecv, device=self.device)\n",
    "        except FileNotFoundError:\n",
    "            return None, 0, np.zeros(4), 0, []\n",
    "        except Exception as err:\n",
    "            print(err)\n",
    "            return None, 0, np.zeros(4), 0, []\n",
    "        # orig_img a numpy array (cv2) in BGR\n",
    "        imagecv = results[0].cpu().orig_img\n",
    "        detection = results[0].cpu().numpy().boxes\n",
    "\n",
    "        # Are there any relevant boxes?\n",
    "        if not len(detection.cls):\n",
    "            return None, 0, np.zeros(4), 0, []\n",
    "        else:\n",
    "            # Yes. Non empty image\n",
    "            pass\n",
    "        # Is there a relevant animal box? \n",
    "        try:\n",
    "            # Yes. Selecting the best animal box\n",
    "            kbox = np.where(detection.cls==0)[0][0]\n",
    "        except IndexError:\n",
    "            # No: Selecting the best box for another category (human, vehicle)\n",
    "            kbox = 0\n",
    "        # categories are 1=animal, 2=person, 3=vehicle and the empty category 0=empty\n",
    "        category = int(detection.cls[kbox]) + 1\n",
    "        box = detection.xyxy[kbox] # xmin, ymin, xmax, ymax\n",
    "        # Is this an animal box ?\n",
    "        if category == 1:\n",
    "            # Yes: cropped image is required for classification\n",
    "            croppedimage = cropSquareCVtoPIL(imagecv, box.copy())\n",
    "        else: \n",
    "            # No: cropped image is not required for classification \n",
    "            croppedimage = None\n",
    "        ## animal count\n",
    "        if category == 1:\n",
    "            count = sum(detection.cls==0) # only above a threshold\n",
    "        else:\n",
    "            count = 0\n",
    "        ## human boxes\n",
    "        ishuman = (detection.cls==1)\n",
    "        if any(ishuman==True):\n",
    "            humanboxes = detection.xyxy[ishuman,]\n",
    "        else:\n",
    "            humanboxes = []\n",
    "        return croppedimage, category, box, count, humanboxes\n",
    "\n",
    "    def merge(self, detector):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddf74e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    def __init__(self, device=None):\n",
    "        self.model = Model(device)\n",
    "        self.model.loadWeights(DFVIT_WEIGHTS)\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.Resize(size=(CROP_SIZE, CROP_SIZE), interpolation=transforms.InterpolationMode.BICUBIC, max_size=None, antialias=None),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=torch.tensor([0.4850, 0.4560, 0.4060]), std=torch.tensor([0.2290, 0.2240, 0.2250]))])\n",
    "\n",
    "    def predictOnBatch(self, batchtensor, withsoftmax=True):\n",
    "        return self.model.predict(batchtensor, withsoftmax)\n",
    "\n",
    "    # croppedimage loaded by PIL\n",
    "    def preprocessImage(self, croppedimage):\n",
    "        preprocessimage = self.transforms(croppedimage)\n",
    "        return preprocessimage.unsqueeze(dim=0)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, device=None):\n",
    "        \"\"\"\n",
    "        Constructor of model classifier\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.base_model = timm.create_model(BACKBONE, pretrained=False,\n",
    "                                            num_classes=len(animals),\n",
    "                                            dynamic_img_size=True)\n",
    "        print(f\"Using {BACKBONE} for classification\")\n",
    "        self.backbone = BACKBONE\n",
    "        self.nbclasses = len(animals)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.base_model(input)\n",
    "        return x\n",
    "\n",
    "    def predict(self, data, withsoftmax=True):\n",
    "        \"\"\"\n",
    "        Predict on test DataLoader\n",
    "        :param test_loader: test dataloader: torch.utils.data.DataLoader\n",
    "        :return: numpy array of predictions without soft max\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        self.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            x = data.to(self.device)\n",
    "            embeddings = self.base_model.forward_features(x)\n",
    "            if withsoftmax:\n",
    "                predictions = self.base_model.forward_head(embeddings).softmax(dim=1)\n",
    "            else:\n",
    "                predictions = self.base_model.forward_head(embeddings)\n",
    "            embeddings = embeddings[:, 0, :]\n",
    "        return predictions.cpu().numpy(), embeddings.cpu().numpy()\n",
    "\n",
    "    def loadWeights(self, path):\n",
    "        \"\"\"\n",
    "        :param path: path of .pt save of model\n",
    "        \"\"\"\n",
    "        if path[-3:] != \".pt\":\n",
    "            path += \".pt\"\n",
    "        try:\n",
    "            params = torch.load(path, map_location=self.device, weights_only=False)\n",
    "            args = params['args']\n",
    "            if self.nbclasses != args['num_classes']:\n",
    "                raise Exception(\"You load a model ({}) that does not have the same number of class\"\n",
    "                                \"({})\".format(args['num_classes'], self.nbclasses))\n",
    "            self.backbone = args['backbone']\n",
    "            self.nbclasses = args['num_classes']\n",
    "            self.load_state_dict(params['state_dict'])\n",
    "        except Exception as e:\n",
    "            print(\"Can't load checkpoint model because :\\n\\n \" + str(e), file=sys.stderr)\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a78b6fcc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BACKBONE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m classifier = \u001b[43mClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m detector = Detector()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mClassifier.__init__\u001b[39m\u001b[34m(self, device)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, device=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mself\u001b[39m.model.loadWeights(DFVIT_WEIGHTS)\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mself\u001b[39m.transforms = transforms.Compose([\n\u001b[32m      6\u001b[39m         transforms.Resize(size=(CROP_SIZE, CROP_SIZE), interpolation=transforms.InterpolationMode.BICUBIC, max_size=\u001b[38;5;28;01mNone\u001b[39;00m, antialias=\u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m      7\u001b[39m         transforms.ToTensor(),\n\u001b[32m      8\u001b[39m         transforms.Normalize(mean=torch.tensor([\u001b[32m0.4850\u001b[39m, \u001b[32m0.4560\u001b[39m, \u001b[32m0.4060\u001b[39m]), std=torch.tensor([\u001b[32m0.2290\u001b[39m, \u001b[32m0.2240\u001b[39m, \u001b[32m0.2250\u001b[39m]))])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mModel.__init__\u001b[39m\u001b[34m(self, device)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[33;03mConstructor of model classifier\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28mself\u001b[39m.base_model = timm.create_model(\u001b[43mBACKBONE\u001b[49m, pretrained=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     25\u001b[39m                                     num_classes=\u001b[38;5;28mlen\u001b[39m(animals),\n\u001b[32m     26\u001b[39m                                     dynamic_img_size=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBACKBONE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for classification\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m \u001b[38;5;28mself\u001b[39m.backbone = BACKBONE\n",
      "\u001b[31mNameError\u001b[39m: name 'BACKBONE' is not defined"
     ]
    }
   ],
   "source": [
    "classifier = Classifier('cuda')\n",
    "detector = Detector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46cd1182",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m batch = []\n\u001b[32m      4\u001b[39m paths = []\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mresults\u001b[49m.iterrows():\n\u001b[32m      7\u001b[39m     image_path = row[\u001b[33m'\u001b[39m\u001b[33mimage_path\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      8\u001b[39m     cropped_image, category, box, count, humanboxes  = detector.bestBoxDetection(image_path)\n",
      "\u001b[31mNameError\u001b[39m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "\n",
    "batch = []\n",
    "paths = []\n",
    "\n",
    "for _, row in results.iterrows():\n",
    "    image_path = row['image_path']\n",
    "    cropped_image, category, box, count, humanboxes  = detector.bestBoxDetection(image_path)\n",
    "    batch.append(classifier.preprocessImage(cropped_image))\n",
    "    paths.append(image_path)\n",
    "\n",
    "    if len(batch) == batch_size:\n",
    "        cropped_images_tensor = torch.zeros((len(batch)))\n",
    "        scores = classifier.predictOnBatch(cropped_images_tensor)\n",
    "\n",
    "        batch = []\n",
    "        paths = []\n",
    "        break\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4239e42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_tensor = torch.ones((1,3,CROP_SIZE,CROP_SIZE))\n",
    "cropped_tensor[0,:,:,:] =  classifier.preprocessImage(cropped_image)\n",
    "scores = classifier.predictOnBatch(cropped_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d60f5fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0.99998,  2.0047e-07,  9.2036e-08,  2.5957e-07,  4.8504e-07,  2.6472e-07,  2.4086e-07,  3.2431e-07,  7.2999e-08,  9.3558e-08,  3.8929e-07,  1.7785e-07,  2.4361e-07,  3.7002e-07,  6.0992e-07,  5.5937e-07,  4.8268e-07,  5.9408e-08,  4.8595e-08,  6.8013e-08,  2.5458e-08,  2.7852e-08,  1.0167e-07,  2.5306e-08,\n",
       "        5.9728e-08,  1.9939e-07,  5.6642e-07,  4.6087e-06,  2.2614e-07,  3.5323e-07,  8.9175e-07,  1.1062e-08,  3.7038e-07,  1.3163e-07,  5.8316e-07,  8.9406e-08,  1.3993e-06,  2.9153e-06], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d07846d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bison</td>\n",
       "      <td>9.999824e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>badger</td>\n",
       "      <td>2.004708e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ibex</td>\n",
       "      <td>9.203561e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>beaver</td>\n",
       "      <td>2.595740e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>red deer</td>\n",
       "      <td>4.850423e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>golden jackal</td>\n",
       "      <td>2.647196e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>chamois</td>\n",
       "      <td>2.408649e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cat</td>\n",
       "      <td>3.243088e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>goat</td>\n",
       "      <td>7.299902e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>roe deer</td>\n",
       "      <td>9.355792e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>dog</td>\n",
       "      <td>3.892945e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>raccoon dog</td>\n",
       "      <td>1.778495e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>fallow deer</td>\n",
       "      <td>2.436117e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>squirrel</td>\n",
       "      <td>3.700185e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>moose</td>\n",
       "      <td>6.099242e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>equid</td>\n",
       "      <td>5.593653e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>genet</td>\n",
       "      <td>4.826816e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>wolverine</td>\n",
       "      <td>5.940758e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>hedgehog</td>\n",
       "      <td>4.859501e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>lagomorph</td>\n",
       "      <td>6.801337e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>wolf</td>\n",
       "      <td>2.545792e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>otter</td>\n",
       "      <td>2.785216e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>lynx</td>\n",
       "      <td>1.016660e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>marmot</td>\n",
       "      <td>2.530629e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>micromammal</td>\n",
       "      <td>5.972787e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>mouflon</td>\n",
       "      <td>1.993923e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>sheep</td>\n",
       "      <td>5.664245e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>mustelid</td>\n",
       "      <td>4.608665e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>bird</td>\n",
       "      <td>2.261438e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>bear</td>\n",
       "      <td>3.532314e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>porcupine</td>\n",
       "      <td>8.917526e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>nutria</td>\n",
       "      <td>1.106183e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>muskrat</td>\n",
       "      <td>3.703782e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>raccoon</td>\n",
       "      <td>1.316294e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>fox</td>\n",
       "      <td>5.831565e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>reindeer</td>\n",
       "      <td>8.940616e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>wild boar</td>\n",
       "      <td>1.399287e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>cow</td>\n",
       "      <td>2.915335e-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          species         score\n",
       "0           bison  9.999824e-01\n",
       "1          badger  2.004708e-07\n",
       "2            ibex  9.203561e-08\n",
       "3          beaver  2.595740e-07\n",
       "4        red deer  4.850423e-07\n",
       "5   golden jackal  2.647196e-07\n",
       "6         chamois  2.408649e-07\n",
       "7             cat  3.243088e-07\n",
       "8            goat  7.299902e-08\n",
       "9        roe deer  9.355792e-08\n",
       "10            dog  3.892945e-07\n",
       "11    raccoon dog  1.778495e-07\n",
       "12    fallow deer  2.436117e-07\n",
       "13       squirrel  3.700185e-07\n",
       "14          moose  6.099242e-07\n",
       "15          equid  5.593653e-07\n",
       "16          genet  4.826816e-07\n",
       "17      wolverine  5.940758e-08\n",
       "18       hedgehog  4.859501e-08\n",
       "19      lagomorph  6.801337e-08\n",
       "20           wolf  2.545792e-08\n",
       "21          otter  2.785216e-08\n",
       "22           lynx  1.016660e-07\n",
       "23         marmot  2.530629e-08\n",
       "24    micromammal  5.972787e-08\n",
       "25        mouflon  1.993923e-07\n",
       "26          sheep  5.664245e-07\n",
       "27       mustelid  4.608665e-06\n",
       "28           bird  2.261438e-07\n",
       "29           bear  3.532314e-07\n",
       "30      porcupine  8.917526e-07\n",
       "31         nutria  1.106183e-08\n",
       "32        muskrat  3.703782e-07\n",
       "33        raccoon  1.316294e-07\n",
       "34            fox  5.831565e-07\n",
       "35       reindeer  8.940616e-08\n",
       "36      wild boar  1.399287e-06\n",
       "37            cow  2.915335e-06"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'species': animals, 'score':scores[0][0]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
